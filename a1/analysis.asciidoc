CSC 321: Assignment 1
====================
Zeeshan Qureshi <g0zee@cdf.toronto.edu>

Train Model
-----------

.Results
[width="95%",options="header",cols="<,^,^,^,^",frame="topbot"]
|========================================================
| Model(d, hid)  | Training | Validation | Test  | Epochs
| model(8, 32)   | 2.869    | 2.894      | 2.891 | 5
| model(8, 256)  | 2.783    | 2.821      | 2.823 | 5
| model(32, 64)  | 2.701    | 2.751      | 2.753 | 8
| model(32, 256) | 2.547    | 2.638      | 2.646 | 8
|========================================================

As we increase the number of dimensions in the distributed representation and
the number of hidden units, the cross entropy error goes down. We run the risk
of over-fitting our network to the data if we increase its capacity
continuously but since our models do early stopping we are disregarding it and
chosing *model(32, 256)* as the best since it has the lowest validaton and test
cross entropy error.

Experiment with Model
---------------------

t-SNE Plot
~~~~~~~~~~

image::plot.png["t-SNE Plot", scaledwidth="100%"]

Closest Words
~~~~~~~~~~~~~

[width="95%", options="header",frame="topbot",halign="center"]
|================================================
| Candidate  | Nearest    | Second    | Third
| company    | department | general   | program
| will       | might      | may       | would
| government | states     | west      | john
| police     | west       | general   | officials
| women      | children   | companies | officials
|================================================

I've only listen some of the more interesting ones here because of the space
constraint. We see that for some words, like _will_, _police_ and _company_
the closest words make a lot of sense and are very likely to appear in the
same sentence. For _women_ and _government_ the closest word makes a lot of
sense but the rest don't seem to be at anything.
