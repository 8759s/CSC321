CSC 321: Assignment 1
====================
Zeeshan Qureshi <g0zee@cdf.toronto.edu>

Train Model
-----------

.Results
[width="95%",options="header",cols="<,^,^,^,^",frame="topbot"]
|========================================================
| Model(d, hid)  | Training | Validation | Test  | Epochs
| model(8, 32)   | 2.869    | 2.894      | 2.891 | 5
| model(8, 256)  | 2.783    | 2.821      | 2.823 | 5
| model(32, 64)  | 2.701    | 2.751      | 2.753 | 8
| model(32, 256) | 2.547    | 2.638      | 2.646 | 8
|========================================================

As we increase the number of dimensions in the distributed representation and
the number of hidden units, the cross entropy error goes down. We run the risk
of over-fitting our network to the data if we increase its capacity
continuously but since our models do early stopping we are disregarding it and
chosing *model(32, 256)* as the best since it has the lowest validaton and test
cross entropy error.

Experiment with Model
---------------------

t-SNE Plot
~~~~~~~~~~

image::plot.png["t-SNE Plot", scaledwidth="100%"]

Closest Words
~~~~~~~~~~~~~

[width="95%", options="header",frame="topbot",halign="center"]
|================================================
| Candidate  | Nearest    | Second    | Third
| company    | department | general   | program
| will       | might      | may       | would
| government | states     | west      | john
| police     | west       | general   | officials
| women      | children   | companies | officials
|================================================

I've only listen some of the more interesting ones here because of the space
constraint. We see that for some words, like _will_, _police_ and _company_
the closest words make a lot of sense and are very likely to appear in the
same sentence. For _women_ and _government_ the closest word makes a lot of
sense but the rest don't seem to be at anything.

Sentence Prediction
~~~~~~~~~~~~~~~~~~~


[width="95%", options="header",frame="topbot",halign="center"]
|================================================
| Sentence             | First | Second | Third
| women also had       | to    | a      | been
| government of united | .     | states | life
|================================================

The model seems to have extracted some features of grammar and semantics of the
english language but not much, for _women also had_ the first 3 predictions are
grammatically correct, but for _government of united_ only the second one makes
sense. If we had mor data to train the network on it would probably have better
prediction of sentences since the 4-gram probabilities would be better for
the not so frequently occurring terms; over here punctuations seem to have
higher probabilities than words.
